{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3aeab5-4219-4bcc-a347-a983985e244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npython test.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributed.pipeline.sync import Pipe\n",
    "\n",
    "from llama_org2 import LLaMA, ModelArgs, Tokenizer, Transformer\n",
    "\n",
    "\n",
    "def load(\n",
    "    # ckpt_dir: str,\n",
    "    # tokenizer_path: str,\n",
    "    # local_rank: int,\n",
    "    # world_size: int,\n",
    ") -> LLaMA:\n",
    "    with open(\"./params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(**params)\n",
    "    model_args.vocab_size = 320000\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    a = time.time()\n",
    "    torch.set_default_device(\"cuda:0\")\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    tf0 = Transformer(model_args, 0)\n",
    "    b = time.time()\n",
    "    print(f\"{b-a}\")\n",
    "    # torch.set_default_device(\"cuda:1\")\n",
    "    tf1 = Transformer(model_args, 1)\n",
    "    # torch.set_default_device(\"cuda:2\")\n",
    "    # tf2 = Transformer(model_args, 2)\n",
    "    # torch.set_default_device(\"cuda:3\")\n",
    "    # tf3 = Transformer(model_args, 3)\n",
    "\n",
    "    torch.set_default_device(\"cuda:1\")\n",
    "    # model = torch.nn.Sequential(tf0, tf1, tf2, tf3)\n",
    "    model = torch.nn.Sequential(tf0)\n",
    "    model = Pipe(model, chunks=8)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main(\n",
    "    # ckpt_dir: str,\n",
    "    # tokenizer_path: str,\n",
    "    ctx_len: int = 60,\n",
    "    follow_len: int = 40,\n",
    "    batch_size: int = 64,\n",
    "):\n",
    "    # Need to initialize RPC framework first.\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    torch.distributed.rpc.init_rpc(\"worker\", rank=0, world_size=1)\n",
    "    model = load()\n",
    "    # time.sleep(20)\n",
    "    # results = generator.prof(ctx_len, follow_len, batch_size)\n",
    "    # torch.save(results, \"./original.pt\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "python test.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48567a08-98ae-46ce-9d1a-4b5cea669d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributed.pipeline.sync import Pipe\n",
    "\n",
    "from llama_org2 import LLaMA, ModelArgs, Tokenizer, Transformer\n",
    "import json\n",
    "\n",
    "with open(\"./params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(**params)\n",
    "model_args.vocab_size = 320000\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "torch.set_default_device(\"cuda:0\")\n",
    "tf0 = Transformer(model_args, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf3580a-6b1b-4f9e-b1a9-7aa6df1b02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0\n",
      "1.4995903968811035\n",
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96568629-9830-4e5e-a08f-6d5e92ec8dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipe(\n",
       "  (partitions): ModuleList(\n",
       "    (0): PipeSequential(\n",
       "      (0): Transformer(\n",
       "        (tok_embeddings): Embedding(320000, 6656)\n",
       "        (layers): ModuleList(\n",
       "          (0-6): 7 x TransformerBlock(\n",
       "            (attention): Attention(\n",
       "              (wq): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "              (wk): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "              (wv): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "              (wo): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "            )\n",
       "            (feed_forward): FeedForward(\n",
       "              (w1): Linear(in_features=6656, out_features=17920, bias=False)\n",
       "              (w2): Linear(in_features=17920, out_features=6656, bias=False)\n",
       "              (w3): Linear(in_features=6656, out_features=17920, bias=False)\n",
       "            )\n",
       "            (attention_norm): RMSNorm()\n",
       "            (ffn_norm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b1f649-9145-4ac6-8af4-e09be747b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].layers[0].attention.wq.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec94d19-43b9-4e0a-98e0-9200afbad24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a12c66-329f-4e4e-930b-e66efabac5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 208.00 MiB (GPU 0; 31.74 GiB total capacity; 30.92 GiB already allocated; 9.62 MiB free; 31.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_args: ModelArgs \u001b[38;5;241m=\u001b[39m ModelArgs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m      5\u001b[0m model_args\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m320000\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tf1 \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/deepfold/user/baehanjin/work/nakta/llama_org2/model.py:254\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params, gpu_num)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    250\u001b[0m layer_vol \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m params\u001b[38;5;241m.\u001b[39mpipeline_size\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[1;32m    252\u001b[0m     layer_vol \u001b[38;5;241m*\u001b[39m gpu_num,\n\u001b[1;32m    253\u001b[0m     layer_vol \u001b[38;5;241m*\u001b[39m (gpu_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m--> 254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(TransformerBlock(layer_id, params))\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/store/deepfold/user/baehanjin/work/nakta/llama_org2/model.py:212\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, layer_id, args)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mn_heads\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m FeedForward(\n\u001b[1;32m    214\u001b[0m     dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdim, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mdim, multiple_of\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmultiple_of\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id \u001b[38;5;241m=\u001b[39m layer_id\n",
      "File \u001b[0;32m/store/deepfold/user/baehanjin/work/nakta/llama_org2/model.py:135\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\n\u001b[1;32m    123\u001b[0m     args\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    124\u001b[0m     args\u001b[38;5;241m.\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# init_method=lambda x: x,\u001b[39;00m\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    131\u001b[0m     (args\u001b[38;5;241m.\u001b[39mmax_batch_size, args\u001b[38;5;241m.\u001b[39mmax_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    132\u001b[0m )\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_local_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama3/lib/python3.10/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 208.00 MiB (GPU 0; 31.74 GiB total capacity; 30.92 GiB already allocated; 9.62 MiB free; 31.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "with open(\"./params.json\", \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(**params)\n",
    "model_args.vocab_size = 320000\n",
    "tf1 = Transformer(model_args, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
